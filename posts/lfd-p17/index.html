<!DOCTYPE html>
<html lang="en" dir=>

<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="This post is a solution to the problem taken from Abu-Mostafa, Yaser S., Malik Magdon-Ismail, and Hsuan-Tien Lin. Learning from data. Vol. 4. New York, NY, USA:: AMLBook, 2012..
Quoted text refers to the original problem statement, verbatim.
For more solutions, see dsevero.com/blog.
Consider leaving a Star if this helps you.
A sample of heads and tails is created by tossing a coin a number of times independently. Assume we have a number of coins that generate different samples independently.">
<meta name="theme-color" content="#FFFFFF"><meta property="og:title" content="Learning From Data, Problem 1.7" />
<meta property="og:description" content="This post is a solution to the problem taken from Abu-Mostafa, Yaser S., Malik Magdon-Ismail, and Hsuan-Tien Lin. Learning from data. Vol. 4. New York, NY, USA:: AMLBook, 2012..
Quoted text refers to the original problem statement, verbatim.
For more solutions, see dsevero.com/blog.
Consider leaving a Star if this helps you.
A sample of heads and tails is created by tossing a coin a number of times independently. Assume we have a number of coins that generate different samples independently." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://dsevero.com/posts/lfd-p17/" /><meta property="article:section" content="posts" />



<title>Learning From Data, Problem 1.7 | Daniel Severo</title>
<link rel="manifest" href="/manifest.json">
<link rel="icon" href="/favicon.png" type="image/x-icon">
<link rel="stylesheet" href="/book.min.a8f172e9514e766ee7562278ee2929cc668544eb977d0ee52069556d89261049.css" integrity="sha256-qPFy6VFOdm7nViJ47ikpzGaFROuXfQ7lIGlVbYkmEEk=">
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-82828654-2', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>
<!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->

  
</head>

<body dir=>
  <input type="checkbox" class="hidden" id="menu-control" />
  <main class="container flex">
    <aside class="book-menu">
      
  <nav>
<h2 class="book-brand">
  <a href="/"><span>Daniel Severo</span>
  </a>
</h2>












  <ul>
<li><a href="/">About me</a></li>
<li><a href="/reading/">Reading</a></li>
<li><a href="/posts/">Writing</a></li>
</ul>






  
<ul>
  
  <li>
    <a href="https://github.com/dsevero" target="_blank" rel="noopener">
        Github
      </a>
  </li>
  
  <li>
    <a href="https://scholar.google.fr/citations?user=5bQjLz4AAAAJ" target="_blank" rel="noopener">
        Google Scholar
      </a>
  </li>
  
  <li>
    <a href="https://www.linkedin.com/in/danielsevero/" target="_blank" rel="noopener">
        LinkedIn
      </a>
  </li>
  
  <li>
    <a href="https://twitter.com/_dsevero" target="_blank" rel="noopener">
        Twitter
      </a>
  </li>
  
  <li>
    <a href="/cv.pdf" target="_blank" rel="noopener">
        CV
      </a>
  </li>
  
</ul>






</nav>




  <script>(function(){var e=document.querySelector("aside.book-menu nav");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script>


 
    </aside>

    <div class="book-page">
      <header class="book-header">
        
  <div class="flex align-center justify-between">
  <label for="menu-control">
    <img src="/svg/menu.svg" class="book-icon" alt="Menu" />
  </label>

  <strong>Learning From Data, Problem 1.7</strong>

  <label for="toc-control">
    
  </label>
</div>


  
 
      </header>

      
      
<article class="markdown">
  <h1>
    <a href="/posts/lfd-p17/">Learning From Data, Problem 1.7</a>
  </h1>
  


  

  


  <p><head>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
  <!-- The loading of KaTeX is deferred to speed up page rendering -->
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
  <!-- To automatically render math in text elements, include the auto-render extension: -->
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
      onload="renderMathInElement(document.body);"></script>
</head>
<p>This post is a solution to the problem taken from <a href="http://www.amlbook.com">Abu-Mostafa, Yaser S., Malik Magdon-Ismail, and Hsuan-Tien Lin. <strong>Learning from data.</strong> Vol. 4. New York, NY, USA:: AMLBook, 2012.</a>.</p>
<p>Quoted text refers to the original problem statement, verbatim.</p>
<p>For more solutions, see <a href="/blog">dsevero.com/blog</a>.</p>
<p>Consider leaving a <span style="text-shadow: none;"><a class="github-button" href="https://github.com/dsevero/dsevero.com" data-icon="octicon-star" data-size="small" data-show-count="true" aria-label="Star this on GitHub">Star</a><script async defer src="https://buttons.github.io/buttons.js"></script></span> if this helps you.</p>
<hr>
<blockquote>
<p>A sample of heads and tails is created by tossing a coin a number of times independently. Assume we have a number of coins that generate different samples independently. For a given coin, let the probability of heads (probability of error) be \(\mu\). The probability of obtaining \(k\) heads in \(N\) tosses of this coin is given by the binomial distribution:
$$ P\left[ k \mid N, \mu \right] = {N\choose k} \mu^k \left(1 - \mu\right)^{N-k} $$
Remember that the training error \(\nu\) is \(\frac{k}{N}\)</p>
</blockquote>
<p>The learning model used in this chapter is the following: assume you have a \(N\) datapoints sampled independently from some unkown distribution \(\mathbf{x}_n \sim P\), targets \(y_n = f(\mathbf{x_n})\) and a set of hypotheses (e.g. machine learning models) \(h \in \mathcal{H}\) of size \(\mid \mathcal{H} \mid = M\). A coin flipping experiment is used to draw conclusions on the accuracy of binary classifiers. The \(n\)-th flip of a coin is the evaluation of some hypothesis \(h\) on point \((\mathbf{x}_n, y_n)\). Heads (numerically, 1) represents an error \(h(\mathbf{x}_n) \neq y_n\), while tails is a successful prediction. In the case of \(M\) coins, we have \(M\) hypotheses and \(NM\) data points \((x_{m,n}, y_{m,n}\))</p>
<p>The objective of this problem is to show that, given a large enough set of hypotheses \(\mathcal{H}\), the probability of obtaining low training error on at least one \(h \in \mathcal{H}\) is high if the data is i.i.d. Therefore, we should be careful when evaluating models even if we have followed the standard train, test and validation split procedure.</p>
<p>How does this translate to practice? Say you have a training dataset \(\mathcal{D}\) and \(M\) models \(h_m \in \mathcal{H}\) that you wish to evaluate. You sample (with replacement) \(N\) points \(\mathbf{x}_{m,n} \in \mathcal{D}\) (e.g. mini-batch training) for each \(h_m\) (i.e. a total of \(NM\) points). What is the probability that at least one hypothesis will have zero in-sample error?</p>
<blockquote>
<p>(a) Assume the sample size \((N)\) is \(10\). If all the coins have \(\mu = 0.05\) compute the probability that at least one coin will have \(v = 0\) for the case of \(1\) coin, \(1,000\) coins, \(1,000,000\) coins. Repeat for \(\mu = 0.8\).</p>
</blockquote>
<p>Let \(k_m\) be the number of heads for each coin. Since \(\nu=0\) implies that \(k=0\), we need to calculate</p>
<p>$$ P\left[ k_1=0 \vee k_2=0 \vee &hellip; k_m=0 \right] = P\left[ \bigvee\limits_{m} k_m = 0 \right]$$</p>
<p>Here, we employ the common trick of computing the probability of the complement</p>
<p>Note that the following step stems from the fact that \(\mathbf{x}_{m,n}\) are independent. If we had used the same set of \(N\) points for all \(h_m\) (i.e. \(\mathbf{x}_{m,n} \rightarrow \mathbf{x}_{n})\), the set of \(k_m\) would not be independent, since looking at a specific \(k_m\) would give you information regarding some other \(k_{m^\prime}\).</p>
<p>$$ \begin{aligned} P\left[ \bigvee\limits_{m} k_m = 0 \right] &amp;= 1 - P\left[ \bigwedge\limits_{m} k_m &gt; 0 \right] \\\ &amp;= 1 - \prod\limits_{m}P\left[ k_m &gt; 0 \right] \end{aligned} $$</p>
<p>Summing over the values of \(k\) and using the fact that \(\sum\limits_{k=0}^N P\left[k\right] = 1\) we can compute</p>
<p>$$ \begin{aligned} P\left[ k_m &gt; 0 \right] &amp;= \sum\limits_{i=1}^N P\left[k\right] \\\ &amp;= \sum\limits_{i=0}^N P\left[k\right] - P\left[0\right] \\\ &amp;= 1 - \left(1 - \mu\right)^N \end{aligned} $$</p>
<p>Thus, resulting in</p>
<p>$$P\left[ \bigvee\limits_{m} k_m = 0 \right] = 1 - \left(  1 - \left(1 - \mu\right)^N \right)^M$$</p>
<p>The result is intuitive. For a single coin, if \(\left(1 - \mu\right)^N\) is the probability that <strong>all</strong> \(N\) flips result in tails, the complement \(1 - \left(1 - \mu\right)^N\) is the probability that <strong>at least one</strong> flip will result in heads. For this to happen to <strong>all</strong> \(M\) coins, we get \(\left(  1 - \left(1 - \mu\right)^N \right)^M\). Similarly, the probability of the complement is \(1 - \left(  1 - \left(1 - \mu\right)^N \right)^M\) and can be interpretated as the probability that <strong>at least one</strong> coin out of \(M\) will have <strong>all</strong> flips out of \(N\) resulting in tails.</p>
<p>Let&rsquo;s take a look at this in python.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#66d9ef">as</span> plt
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> pandas <span style="color:#66d9ef">as</span> pd
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">prob_zero_error</span>(μ: <span style="color:#e6db74">&#39;true probability of error&#39;</span>,
</span></span><span style="display:flex;"><span>                    M: <span style="color:#e6db74">&#39;number of hypotheses&#39;</span>,
</span></span><span style="display:flex;"><span>                    N: <span style="color:#e6db74">&#39;number of data points&#39;</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> <span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> μ)<span style="color:#f92672">**</span>N)<span style="color:#f92672">**</span>M
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>d <span style="color:#f92672">=</span> [{<span style="color:#e6db74">&#39;μ&#39;</span>: μ, 
</span></span><span style="display:flex;"><span>      <span style="color:#e6db74">&#39;M&#39;</span>: M, 
</span></span><span style="display:flex;"><span>      <span style="color:#e6db74">&#39;p&#39;</span>: prob_zero_error(μ, M, N<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>)} 
</span></span><span style="display:flex;"><span>     <span style="color:#66d9ef">for</span> μ <span style="color:#f92672">in</span> [<span style="color:#ae81ff">0.05</span>, <span style="color:#ae81ff">0.8</span>] 
</span></span><span style="display:flex;"><span>     <span style="color:#66d9ef">for</span> M <span style="color:#f92672">in</span> [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1_000</span>, <span style="color:#ae81ff">1_000_000</span>]]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>pd<span style="color:#f92672">.</span>DataFrame(d)<span style="color:#f92672">.</span>pivot(<span style="color:#e6db74">&#39;M&#39;</span>, <span style="color:#e6db74">&#39;μ&#39;</span>, <span style="color:#e6db74">&#39;p&#39;</span>)<span style="color:#f92672">.</span>to_html()
</span></span></code></pre></div><table>
  <thead>
    <tr style="text-align: right;">
      <th>μ</th>
      <th>0.05</th>
      <th>0.5</th>
      <th>0.8</th>
    </tr>
    <tr>
      <th>M</th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.598737</td>
      <td>0.000977</td>
      <td>1.024000e-07</td>
    </tr>
    <tr>
      <th>1000</th>
      <td>1.000000</td>
      <td>0.623576</td>
      <td>1.023948e-04</td>
    </tr>
    <tr>
      <th>1000000</th>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>9.733159e-02</td>
    </tr>
  </tbody>
</table>
<p>We&rsquo;ve included the results for \(\mu = 0.5\), which represents a reasonable error rate for an untrained binary classification model. The middle cell tells us that a sample of size \(NM = 10^4\) evaluated on \(M=10^3\) hypotheses (with \(10\) samples each) has a \(62.36\%\) chance of at least one hypothesis having error zero.</p>
<p>Let&rsquo;s take a look at the asymptotic properties of \(P(N,M) = 1 - \left(  1 - \left(1 - \mu\right)^N \right)^M\) for \(\mu \in (0, 1)\).</p>
<p>$$\lim\limits_{M \rightarrow \infty} P(N,M) = 1$$
$$\lim\limits_{N \rightarrow \infty} P(N,M) = 0$$</p>
<p>Intuitvely, evaluating on more datapoints \(N\) should make it harder for all points (coins) to have zero error (tails) for any number of hypotheses. Using a larger hypothesis set \(\mid\mathcal{H}\mid = M\) is analogous to brute forcing the appearance of \(k=0\) through repetitive attempts.</p>
<p>If we want to bound this probability (for the sake of sanity) to some value \(\lambda\), how should we chose \(M\) and \(N\)? Solving independently for \(N\) and \(M\) in \(P(N,M) \leq \lambda\)</p>
<p>$$M \leq \frac{log\left(1 - \lambda\right)}{log\left(1 - \left(1 - \mu\right)^N\right)}$$
$$N \geq \frac{log\left(1 - \sqrt[M]{1 - \lambda} \right)}{log\left(1 - \mu\right)}$$</p>
<p>We can use these result to calculate how fast the number of hypotheses \(M\) can grow with respect to the number of datapoints \(N\) for a fixed probability of zero error \(\lambda\), and vice-versa.</p>
<blockquote>
<p>(b) For the case \(N = 6\) and \(2\) coins with \(\mu = 0.5\) for both coins, plot the probability \(P[ \max\limits_i \mid \nu_i - \mu_i \mid &gt; \epsilon ]\) for \(\epsilon\) in the range \([0, 1]\) (the max is over coins). On the same plot show the bound that would be obtained using the Hoeffding Inequality. Remember that for a single coin, the Hoeffding bound is $$P[\mid \nu- \mu \mid &gt; \epsilon ] \leq 2e^{-2N\epsilon^2}$$</p>
</blockquote>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#66d9ef">as</span> plt
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>style<span style="color:#f92672">.</span>use(<span style="color:#e6db74">&#39;ggplot&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>N <span style="color:#f92672">=</span> <span style="color:#ae81ff">6</span>
</span></span><span style="display:flex;"><span>M <span style="color:#f92672">=</span> <span style="color:#ae81ff">2</span>
</span></span><span style="display:flex;"><span>μ <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.5</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">hoeffding_bound</span>(ε, N, M<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> <span style="color:#ae81ff">2</span><span style="color:#f92672">*</span>M<span style="color:#f92672">*</span>np<span style="color:#f92672">.</span>exp(<span style="color:#f92672">-</span><span style="color:#ae81ff">2</span><span style="color:#f92672">*</span>N<span style="color:#f92672">*</span>ε<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">P</span>(N, M, ε_space, μ):
</span></span><span style="display:flex;"><span>    k <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>binomial(n<span style="color:#f92672">=</span>N,
</span></span><span style="display:flex;"><span>                           p<span style="color:#f92672">=</span>μ,
</span></span><span style="display:flex;"><span>                           size<span style="color:#f92672">=</span>(<span style="color:#ae81ff">1_000</span>, M))
</span></span><span style="display:flex;"><span>    P <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>abs(k<span style="color:#f92672">/</span>N <span style="color:#f92672">-</span> μ)<span style="color:#f92672">.</span>max(axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> [(P <span style="color:#f92672">&gt;</span> ε)<span style="color:#f92672">.</span>mean() <span style="color:#66d9ef">for</span> ε <span style="color:#f92672">in</span> ε_space]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>ε_space <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>linspace(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">100</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">12</span>,<span style="color:#ae81ff">5</span>))
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>plot(ε_space, hoeffding_bound(ε_space, N), <span style="color:#e6db74">&#39;--&#39;</span>,
</span></span><span style="display:flex;"><span>         ε_space, hoeffding_bound(ε_space, N, M<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>), <span style="color:#e6db74">&#39;--&#39;</span>, 
</span></span><span style="display:flex;"><span>         ε_space, P(<span style="color:#ae81ff">6</span>, <span style="color:#ae81ff">2</span>, ε_space, μ),
</span></span><span style="display:flex;"><span>         ε_space, P(<span style="color:#ae81ff">6</span>, <span style="color:#ae81ff">10</span>, ε_space, μ));
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#39;Average over $1000$ iterations of</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#39;</span>
</span></span><span style="display:flex;"><span>          <span style="color:#e6db74">&#39;$\max \{ \mid k_1/6 - 0.5 \mid,&#39;</span>
</span></span><span style="display:flex;"><span>          <span style="color:#e6db74">&#39;\mid k_2/6 - 0.5 \mid\} &gt; \epsilon $</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#39;</span>,
</span></span><span style="display:flex;"><span>          fontsize<span style="color:#f92672">=</span><span style="color:#ae81ff">20</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>legend([<span style="color:#e6db74">&#39;Hoeffding Bound &#39;</span>
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#39;($M=1 </span><span style="color:#ae81ff">\\</span><span style="color:#e6db74">rightarrow 2e^{-12\epsilon^2}$)&#39;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#39;Hoeffding Bound &#39;</span>
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#39;($M=3 </span><span style="color:#ae81ff">\\</span><span style="color:#e6db74">rightarrow 6e^{-12\epsilon^2}$)&#39;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#39;$M=2$&#39;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#39;$M=3$&#39;</span>], fontsize<span style="color:#f92672">=</span><span style="color:#ae81ff">18</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>yticks(fontsize<span style="color:#f92672">=</span><span style="color:#ae81ff">18</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>xticks(fontsize<span style="color:#f92672">=</span><span style="color:#ae81ff">18</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>ylim(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#39;$\epsilon$&#39;</span>, fontsize<span style="color:#f92672">=</span><span style="color:#ae81ff">20</span>);
</span></span></code></pre></div><p><img src="b.png" alt="b.png" /></p>
<p>Notice how the Hoeffding Bound is violated for \(M=3\) if multiple hypotheses are not properly accounted for.</p>
<!---
> [Hint: Use $P\[A\\space\\text{or}\\space B\] = P\[A\] + P\[B\] \\space \\space \\space \\space P\[A\\space\\text{and}\\space B\] = P\[A\] + P\[B\] - P\[A\] P\[B\]$, where the last equality follows by independence, to evaluate $P\[\\max \\dots \]]$.
-->
<hr>
<p>Questions, suggestions or corrections? Message me on <a href="https://twitter.com/_dsevero">twitter</a> or create a pull request at <a href="https://github.com/dsevero/dsevero.com">dsevero/dsevero.com</a></p>
<p>Consider leaving a <span style="text-shadow: none;"><a class="github-button" href="https://github.com/dsevero/dsevero.com" data-icon="octicon-star" data-size="small" data-show-count="true" aria-label="Star this on GitHub">Star</a><script async defer src="https://buttons.github.io/buttons.js"></script></span> if this helps you.</p>
<hr>
</p>
</article>
 
      

      <footer class="book-footer">
        
  <div class="flex flex-wrap justify-between">





</div>

 
        
      </footer>

      
  
 

      <label for="menu-control" class="hidden book-menu-overlay"></label>
    </div>

    
  </main>

  
</body>

</html>












